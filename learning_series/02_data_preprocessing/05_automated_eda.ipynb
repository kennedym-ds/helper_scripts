{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated EDA with Helper Tools\n",
    "\n",
    "This notebook demonstrates how to use the existing `auto_eda.py` module from the helper_scripts repository to perform comprehensive exploratory data analysis quickly and efficiently.\n",
    "\n",
    "## Learning Objectives\n",
    "- Learn to use the GeneralEDA class for automated analysis\n",
    "- Generate comprehensive data reports automatically\n",
    "- Understand advanced EDA techniques and their applications\n",
    "- Build reusable EDA workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import\n",
    "\n",
    "First, let's import the necessary libraries and the auto_eda module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import the GeneralEDA class from the repository\n",
    "import sys\n",
    "sys.path.append('../../')  # Add path to access the auto_eda module\n",
    "from auto_eda import GeneralEDA\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"üìä Ready to perform automated EDA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Dataset\n",
    "\n",
    "Let's create a sample dataset to demonstrate the EDA capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistic sample dataset for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic customer data\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.normal(35, 12, n_samples).astype(int),\n",
    "    'income': np.random.exponential(50000, n_samples),\n",
    "    'spending_score': np.random.beta(2, 5, n_samples) * 100,\n",
    "    'education_level': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "                                       n_samples, p=[0.4, 0.35, 0.2, 0.05]),\n",
    "    'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], \n",
    "                            n_samples, p=[0.3, 0.25, 0.2, 0.15, 0.1]),\n",
    "    'purchase_frequency': np.random.poisson(3, n_samples),\n",
    "    'satisfaction_rating': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.05, 0.1, 0.2, 0.4, 0.25]),\n",
    "    'signup_date': pd.date_range('2020-01-01', periods=n_samples, freq='D')[:n_samples]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add some missing values to make it realistic\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "df.loc[missing_indices, 'income'] = np.nan\n",
    "\n",
    "missing_indices = np.random.choice(df.index, size=int(0.03 * len(df)), replace=False)\n",
    "df.loc[missing_indices, 'education_level'] = np.nan\n",
    "\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(df.index, size=20, replace=False)\n",
    "df.loc[outlier_indices, 'income'] = df.loc[outlier_indices, 'income'] * 10\n",
    "\n",
    "print(f\"Dataset created with {len(df)} rows and {len(df.columns)} columns\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize GeneralEDA Class\n",
    "\n",
    "Now let's initialize the GeneralEDA class and start our automated analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GeneralEDA class\n",
    "eda = GeneralEDA(df)\n",
    "\n",
    "print(\"üîß GeneralEDA initialized successfully!\")\n",
    "print(\"üìã Available methods:\")\n",
    "\n",
    "# List available methods\n",
    "methods = [method for method in dir(eda) if not method.startswith('_')]\n",
    "for i, method in enumerate(methods, 1):\n",
    "    print(f\"{i:2d}. {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Integrity Validation\n",
    "\n",
    "Let's start with validating the integrity of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data integrity\n",
    "print(\"üîç Validating data integrity...\\n\")\n",
    "eda.validate_data_integrity()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Data integrity validation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Data Information\n",
    "\n",
    "Get comprehensive information about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic data information\n",
    "print(\"üìä Basic Data Information:\\n\")\n",
    "eda.data_info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Basic data information completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Summary\n",
    "\n",
    "Generate comprehensive statistical summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate statistical summary\n",
    "print(\"üìà Statistical Summary:\\n\")\n",
    "eda.statistical_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Statistical summary completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handle Missing Values\n",
    "\n",
    "Automatically detect and handle missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values before handling\n",
    "print(\"‚ùå Missing values before handling:\")\n",
    "print(eda.df.isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\nüîß Handling missing values...\")\n",
    "eda.handle_missing_values(strategy='mean')  # For numerical columns\n",
    "\n",
    "# Check missing values after handling\n",
    "print(\"\\n‚úÖ Missing values after handling:\")\n",
    "print(eda.df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Missing value handling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handle Duplicates\n",
    "\n",
    "Detect and remove duplicate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(f\"üìã Rows before duplicate handling: {len(eda.df)}\")\n",
    "\n",
    "# Handle duplicates\n",
    "eda.handle_duplicates()\n",
    "\n",
    "print(f\"üìã Rows after duplicate handling: {len(eda.df)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Duplicate handling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlier Detection and Handling\n",
    "\n",
    "Automatically detect and handle outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers using Z-score method\n",
    "print(\"üéØ Handling outliers using Z-score method...\\n\")\n",
    "eda.handle_outliers(method='zscore', threshold=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Outlier handling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering\n",
    "\n",
    "Automatically create new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shape before feature engineering\n",
    "print(f\"üìä Columns before feature engineering: {len(eda.df.columns)}\")\n",
    "print(f\"Columns: {list(eda.df.columns)}\")\n",
    "\n",
    "# Perform feature engineering\n",
    "print(\"\\nüî® Performing automated feature engineering...\")\n",
    "eda.feature_engineering()\n",
    "\n",
    "# Get shape after feature engineering\n",
    "print(f\"\\nüìä Columns after feature engineering: {len(eda.df.columns)}\")\n",
    "print(f\"New columns added: {len(eda.df.columns) - len(df.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Feature engineering completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Scaling\n",
    "\n",
    "Apply feature scaling to numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature scaling\n",
    "print(\"‚öñÔ∏è Applying feature scaling (standard scaler)...\")\n",
    "eda.feature_scaling(method='standard')\n",
    "\n",
    "# Show summary statistics after scaling\n",
    "print(\"\\nüìä Summary statistics after scaling:\")\n",
    "numerical_cols = eda.df.select_dtypes(include=[np.number]).columns\n",
    "print(eda.df[numerical_cols].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Feature scaling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Anomaly Detection\n",
    "\n",
    "Detect anomalies in the dataset using Isolation Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and visualize anomalies\n",
    "print(\"üîç Detecting anomalies using Isolation Forest...\")\n",
    "eda.detect_and_visualize_anomalies(contamination=0.05)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Anomaly detection completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Generate Comprehensive Reports\n",
    "\n",
    "Create automated reports for the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pandas profiling report (commented out to avoid large output)\n",
    "# This would create a comprehensive HTML report\n",
    "print(\"üìã Generating comprehensive data profiling report...\")\n",
    "print(\"(This would normally create an HTML report with detailed analysis)\")\n",
    "\n",
    "# Uncomment the following lines to generate actual report:\n",
    "# eda.generate_pandas_profiling_report(\n",
    "#     output_file=\"customer_data_profile.html\",\n",
    "#     sample_fraction=0.1,  # Use 10% sample for faster processing\n",
    "#     title=\"Customer Data Analysis Report\"\n",
    "# )\n",
    "\n",
    "print(\"‚úÖ Report generation configured (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Analysis Report\n",
    "\n",
    "Save a text summary of the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive text report\n",
    "print(\"üíæ Saving comprehensive EDA report...\")\n",
    "eda.save_report(filepath=\"customer_eda_report.txt\")\n",
    "\n",
    "print(\"‚úÖ Report saved successfully!\")\n",
    "print(\"üìÅ Check 'customer_eda_report.txt' for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Get Transformed Dataset\n",
    "\n",
    "Retrieve the final processed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the transformed dataset\n",
    "transformed_df = eda.get_dataframe()\n",
    "\n",
    "print(\"üìä Final Dataset Summary:\")\n",
    "print(f\"   Original shape: {df.shape}\")\n",
    "print(f\"   Transformed shape: {transformed_df.shape}\")\n",
    "print(f\"   Columns added: {transformed_df.shape[1] - df.shape[1]}\")\n",
    "print(f\"   Missing values: {transformed_df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nüìã Final columns:\")\n",
    "for i, col in enumerate(transformed_df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset ready for machine learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Custom Analysis Functions\n",
    "\n",
    "Let's create some custom analysis functions that extend the GeneralEDA capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_relationships(df, target_col=None):\n",
    "    \"\"\"Analyze relationships between categorical variables\"\"\"\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    if len(categorical_cols) < 2:\n",
    "        print(\"Need at least 2 categorical columns for relationship analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîó Categorical Variable Relationships:\")\n",
    "    \n",
    "    for i, col1 in enumerate(categorical_cols):\n",
    "        for col2 in categorical_cols[i+1:]:\n",
    "            # Create contingency table\n",
    "            contingency = pd.crosstab(df[col1], df[col2])\n",
    "            \n",
    "            # Calculate Cram√©r's V (measure of association)\n",
    "            chi2 = contingency.values\n",
    "            n = contingency.sum().sum()\n",
    "            \n",
    "            print(f\"\\n{col1} vs {col2}:\")\n",
    "            print(contingency)\n",
    "\n",
    "def create_feature_interaction_plot(df, feature1, feature2, target=None):\n",
    "    \"\"\"Create interaction plots between features\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(df[feature1], df[feature2], alpha=0.6)\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.title(f'{feature1} vs {feature2}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution plots\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(df[feature1], bins=30, alpha=0.7, label=feature1)\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of {feature1}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(df[feature2], bins=30, alpha=0.7, label=feature2, color='orange')\n",
    "    plt.xlabel(feature2)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of {feature2}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Apply custom analysis\n",
    "print(\"üé® Applying custom analysis functions...\")\n",
    "\n",
    "# Analyze categorical relationships\n",
    "analyze_categorical_relationships(df)\n",
    "\n",
    "# Create feature interaction plots for numerical features\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns[:2]\n",
    "if len(numerical_features) >= 2:\n",
    "    create_feature_interaction_plot(df, numerical_features[0], numerical_features[1])\n",
    "\n",
    "print(\"\\n‚úÖ Custom analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary and Next Steps\n",
    "\n",
    "Let's summarize what we've accomplished with the automated EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ AUTOMATED EDA COMPLETED!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ Tasks Completed:\")\n",
    "tasks = [\n",
    "    \"Data integrity validation\",\n",
    "    \"Basic data information analysis\",\n",
    "    \"Statistical summary generation\",\n",
    "    \"Missing value handling\",\n",
    "    \"Duplicate detection and removal\",\n",
    "    \"Outlier detection and treatment\",\n",
    "    \"Automated feature engineering\",\n",
    "    \"Feature scaling and normalization\",\n",
    "    \"Anomaly detection with visualization\",\n",
    "    \"Comprehensive report generation\",\n",
    "    \"Custom analysis functions\"\n",
    "]\n",
    "\n",
    "for i, task in enumerate(tasks, 1):\n",
    "    print(f\"{i:2d}. {task}\")\n",
    "\n",
    "print(\"\\nüìä Dataset Transformation Summary:\")\n",
    "print(f\"   ‚Ä¢ Original columns: {len(df.columns)}\")\n",
    "print(f\"   ‚Ä¢ Final columns: {len(transformed_df.columns)}\")\n",
    "print(f\"   ‚Ä¢ Features added: {len(transformed_df.columns) - len(df.columns)}\")\n",
    "print(f\"   ‚Ä¢ Missing values eliminated: {df.isnull().sum().sum()} ‚Üí {transformed_df.isnull().sum().sum()}\")\n",
    "print(f\"   ‚Ä¢ Dataset ready for ML: ‚úÖ\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Use the transformed dataset for machine learning models\")\n",
    "print(\"   2. Apply the same EDA pipeline to new datasets\")\n",
    "print(\"   3. Customize the analysis based on specific domain requirements\")\n",
    "print(\"   4. Explore advanced feature engineering techniques\")\n",
    "print(\"   5. Move to Module 3: Supervised Learning\")\n",
    "\n",
    "print(\"\\nüöÄ You're now ready to build machine learning models with clean, well-prepared data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Automated Analysis**: How to use existing tools to quickly analyze datasets\n",
    "2. **Data Quality**: Importance of data validation and integrity checks\n",
    "3. **Missing Value Strategies**: Different approaches for different data types\n",
    "4. **Outlier Detection**: Multiple methods for identifying unusual data points\n",
    "5. **Feature Engineering**: Automated creation of new meaningful features\n",
    "6. **Scaling and Normalization**: Preparing data for machine learning algorithms\n",
    "7. **Anomaly Detection**: Using unsupervised methods to find unusual patterns\n",
    "8. **Report Generation**: Creating comprehensive documentation of analysis\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Always validate data integrity before analysis\n",
    "- Handle missing values appropriately for each data type\n",
    "- Visualize distributions and relationships\n",
    "- Document all preprocessing steps\n",
    "- Save intermediate results for reproducibility\n",
    "- Use automation for routine tasks, but understand what's happening\n",
    "\n",
    "### Exercise for Practice:\n",
    "\n",
    "Try applying this automated EDA workflow to different types of datasets:\n",
    "- Time series data (stock prices, sensor readings)\n",
    "- Text data (customer reviews, social media posts)\n",
    "- Image metadata (if working with computer vision datasets)\n",
    "- Mixed data types (combination of numerical, categorical, and text)\n",
    "\n",
    "The GeneralEDA class provides a solid foundation that you can extend and customize for specific use cases!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}